{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Import Libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import FastText\n",
    "from gensim.models import Word2Vec\n",
    "import textdistance\n",
    "import ast\n",
    "import random\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "source": [
    "## Standard Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "72281\n"
     ]
    }
   ],
   "source": [
    "### Read KBBI. collected from kbbi kemdikbud (not all words in kbbi kemdikbud is successfully scrapped)\n",
    "\n",
    "kbbi = open(\"data/kbbi.txt\").read().splitlines()\n",
    "kbbi = set(kbbi)\n",
    "print(len(kbbi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixWords = {\"begimana\": \"bagaimana\", \"kalo\": \"kalau\",  'telpon': 'telepon',\n",
    "            'kos':'indekos', 'cek': 'mengecek','toples': 'stoples','asik': 'asyik',\n",
    "            'silahkan': 'silakan','rame': 'ramai','brewok': 'berewok','pepet': 'memepet',\n",
    "            'duh' :'aduh','setel' :'menyetel','kebut' :'mengebut','umpet': 'mengumpet',\n",
    "            'cabut': 'mencabut','rumpi': 'merumpi','detil': 'detail', 'rame' : 'ramai','angus':'hangus',\n",
    "           'berantak':'berantakan', 'tropi':'trofi','hapal':'hafal','brewok':'berewok','keprek':'mengeprek',\n",
    "           'mantol':'mantel','nasehat':'nasihat','ustad':'ustaz','teraweh':'tarawih','sukur':'syukur',\n",
    "            'emak':'mak','adzan':'azan','telpon':'telepon','puteri':'putri','kecoa':'kecoak','perduli':'peduli',\n",
    "            'impi':'mengimpikan','gue':'aku','kaco':'kacau', 'adhan':'azan'\n",
    "           }\n",
    "\n",
    "### few words that are \"ragam tak baku\" are included in kbbi so fixWords function to help clean these words\n",
    "fixWords = pd.read_csv('data/fixWords.csv')\n",
    "fixWords = dict(zip(fixWords.ragamTB, fixWords.B))\n",
    "\n",
    "def fixdWord(inKbbiNotBaku):\n",
    "    global fixWords\n",
    "    try:\n",
    "        return [inKbbiNotBaku[0], fixWords[inKbbiNotBaku[1]]]\n",
    "    except:\n",
    "        return inKbbiNotBaku"
   ]
  },
  {
   "source": [
    "## Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find t1 candidates that close to nsw word\n",
    "\n",
    "def findCandidates(word, model, t1):\n",
    "    '''\n",
    "    generate t1 candidates that most similar to the nsw word using the model\n",
    "    word    : nsw word --string\n",
    "    model   : either word2vec model or fasttext model --.model\n",
    "    t1      : how many candidates want to generate\n",
    "    return  : candidates --list of string\n",
    "    '''\n",
    "    global kbbi\n",
    "    try:\n",
    "        candidates = model.wv.most_similar(word, topn = t1)\n",
    "        candidates = [i for i in candidates if i[0] in kbbi and len(i[0])>1]\n",
    "    except:\n",
    "        return []\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find closest candidates based on Jaro-Winkler and Levenshtein distance\n",
    "\n",
    "def emb_JWdist_LevDist(listofwords, referenceWord, t3):\n",
    "    '''\n",
    "    listofwords     : candidates -- list of string\n",
    "    Find the most similar word to referenceWord(nsw) in listofwords\n",
    "    referenceWord   : non standard word -- string\n",
    "    t3              : minimun score to determined that two words are similar (jaro-winkler and levenshtein)\n",
    "    output          : either [0, referenceWord] or [3, standar word]. 3 represents that the output is                       received based on jaro winkler and levenshtein similarities\n",
    "    '''\n",
    "    if listofwords!=[]:\n",
    "        JWdist = [ textdistance.jaro_winkler.normalized_similarity(i[0],referenceWord) for i in listofwords ]\n",
    "        LevDist = [ textdistance.levenshtein.normalized_similarity(i[0],referenceWord) for i in listofwords ]\n",
    "        distance = [0.5*listofwords[i][1] + 0.25*JWdist[i] + 0.25*LevDist[i] for i in range(len(listofwords))]\n",
    "        choosenWord = [[listofwords[i][0],distance[i]] for i in range(len(listofwords)) if distance[i] > t3]    \n",
    "        if choosenWord != []:\n",
    "            ans = max(choosenWord, key=lambda x:x[1])\n",
    "            return [3,ans[0]]\n",
    "        else:\n",
    "            return [0, referenceWord]\n",
    "    else:\n",
    "        return [0, referenceWord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Find the most similar word to nsw in candidates\n",
    "def modelSim(nsw,t2,t3,candidates):\n",
    "    '''\n",
    "    Find the most similar word to nsw in candidates\n",
    "    nsw     : non standard word -- string\n",
    "    t2      : minimun score to determined that two words are similar (only based on cosine similarity)              -- int\n",
    "    t3      : minimun score to determined that two words are similar (jaro-winkler and levenshtein)\n",
    "            -- int\n",
    "    output  : either [2, standar word] or call another function. 2 represents that the output is                    received based on cosine sim only -- list\n",
    "    '''\n",
    "    if candidates[0][1] > t2:\n",
    "        return [2, candidates[0][0]]\n",
    "    else:\n",
    "        return emb_JWdist_LevDist(candidates, nsw, t3)\n",
    "\n",
    "    \n",
    "def normalize(nsw,t2,t3,candidates):\n",
    "    '''\n",
    "    Check wether normalize the nsw or not\n",
    "    nsw     : non standard word -- string\n",
    "    t2      : minimun score to determined that two words are similar (only based on cosine similarity)              -- int\n",
    "    t3      : minimun score to determined that two words are similar (jaro-winkler and levenshtein)\n",
    "            -- int\n",
    "    candidates : candidates taken from findCandidates function\n",
    "    '''\n",
    "    global kbbi\n",
    "    if nsw in kbbi or len(nsw)==1 :\n",
    "        return [0,nsw]\n",
    "    if candidates!=[]:\n",
    "        return modelSim(nsw,t2,t3,candidates)\n",
    "    else:\n",
    "        return [0, nsw]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeWord(nsw,t1,t2,t3,model,modelType):\n",
    "    '''\n",
    "    Combine all the functions to get the result\n",
    "    result  : List of indicator number and the standar word (sw).\n",
    "    '''\n",
    "    if modelType == 'ft':\n",
    "        model = FastText.load(model)\n",
    "    elif modelType == 'wtv':\n",
    "        model = Word2Vec.load(model)\n",
    "    candidates = findCandidates(nsw, model, t1)\n",
    "    sw_predicted = normalize(nsw,t2,t3,candidates)\n",
    "    sw_predicted = fixdWord(sw_predicted)\n",
    "    return sw_predicted\n"
   ]
  },
  {
   "source": [
    "## Example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 7.97 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[3, 'banyak']"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "\n",
    "model = \"data\\\\fastTextmodelJktnonEnglish_alpha025_window10_epoch300_size300.model\"\n",
    "NormalizeWord('buanyak',30,0.85,0.55,model,modelType='ft')\n",
    "\n",
    "### disclaimer : this model is too big, to download the model contact me via email hardianarafik@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}